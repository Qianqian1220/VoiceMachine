import os
import pandas as pd
import jiwer
import ast
import numpy as np
from scipy.optimize import linear_sum_assignment

# 1ï¸âƒ£ è®¾å®šå›ºå®šå­˜å‚¨ç›®å½•
output_dir = "result_3__acc"
os.makedirs(output_dir, exist_ok=True)

# 2ï¸âƒ£ åŠ è½½æ ‡å‡†ç­”æ¡ˆ data_b.csv
ref_df = pd.read_csv("data_b.csv", dtype={"ID": str})  # ç¡®ä¿ ID ä¸ºå­—ç¬¦ä¸²
ref_df.rename(columns={"ID": "uttid", "Sentence": "reference"}, inplace=True)
reference_texts = ref_df["reference"].tolist()

if len(reference_texts) != 50:
    raise ValueError(f"âŒ å‚è€ƒæ–‡æœ¬æ•°é‡é”™è¯¯ï¼Œåº”ä¸º 50ï¼Œå½“å‰ä¸º {len(reference_texts)}")

# 3ï¸âƒ£ éå† asr_results ç›®å½•ä¸­çš„æ‰€æœ‰ CSV è¯†åˆ«ç»“æœ
asr_results_dir = "results_3_"
asr_files = [f for f in os.listdir(asr_results_dir) if f.endswith(".csv")]
if not asr_files:
    raise ValueError("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½• ASR è¯†åˆ«ç»“æœæ–‡ä»¶ï¼Œé€€å‡ºã€‚")

for asr_file in asr_files:
    sub_dir = os.path.splitext(asr_file)[0]  # æ–‡ä»¶åï¼ˆå»æ‰ .csvï¼‰
    asr_csv_path = os.path.join(asr_results_dir, asr_file)
    asr_df = pd.read_csv(asr_csv_path, dtype={"uttid": str})

    if len(asr_df) != 50:
        raise ValueError(f"âŒ {asr_file} å¥å­æ•°é‡é”™è¯¯ï¼Œåº”ä¸º 50ï¼Œå½“å‰: {len(asr_df)}")

    # 4ï¸âƒ£ è§£æ ASR ç»“æœï¼Œæå–æ¯ä¸€å¥çš„è¯†åˆ«æ–‡æœ¬
    asr_texts = []
    uttid_list = asr_df["uttid"].tolist()  # ä¿å­˜ uttid é¡ºåº
    for _, row in asr_df.iterrows():
        raw_transcription = row["transcription"]
        try:
            if isinstance(raw_transcription, str):
                transcription_data = ast.literal_eval(raw_transcription)
                asr_text = transcription_data.get("text", "").strip()
                asr_texts.append(asr_text if asr_text else "ç©º")
            else:
                asr_texts.append("ç©º")
        except Exception as e:
            print(f"âŒ è§£æ {row['uttid']} å¤±è´¥: {e}")
            asr_texts.append("ç©º")

    if len(asr_texts) != 50:
        raise ValueError(f"âŒ è§£æå‡ºçš„ ASR ç»“æœæ•°é‡é”™è¯¯ï¼Œåº”ä¸º 50ï¼Œå½“å‰: {len(asr_texts)}")

    # 5ï¸âƒ£ é¢„å¤„ç†ï¼šå»æ‰æ ‡ç‚¹å’Œå¤šä½™ç©ºæ ¼
    transform = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.RemoveMultipleSpaces()])
    processed_ref = [transform(s.strip()) for s in reference_texts]
    processed_asr = [transform(s.strip()) for s in asr_texts]

    # å»é™¤ç©ºæ ¼ä¾¿äºè®¡ç®—å­—ç¬¦ CERï¼ˆä¸­æ–‡é€šå¸¸ä¸ç”¨ç©ºæ ¼åˆ†è¯ï¼Œè¿™é‡Œç›´æ¥ç§»é™¤ç©ºæ ¼ï¼‰
    processed_ref_chars = [s.replace(" ", "") for s in processed_ref]
    processed_asr_chars = [s.replace(" ", "") for s in processed_asr]

    # 6ï¸âƒ£ æ„å»º 50x50 çš„ä»£ä»·çŸ©é˜µï¼ˆcost matrixï¼‰ï¼Œcost ä¸ºå‚è€ƒå¥ä¸ ASR å¥ä¹‹é—´çš„ CER
    cost_matrix = np.zeros((50, 50))
    for i in range(50):
        for j in range(50):
            cer_val = jiwer.cer(processed_ref_chars[i], processed_asr_chars[j])
            cost_matrix[i, j] = cer_val if cer_val <= 1.0 else 1.0

    # 7ï¸âƒ£ ç”¨ Hungarian ç®—æ³•æ±‚è§£æœ€ä¼˜åˆ†é…é—®é¢˜
    row_ind, col_ind = linear_sum_assignment(cost_matrix)
    # row_ind å¯¹åº”å‚è€ƒå¥ç´¢å¼•ï¼Œ col_ind å¯¹åº” ASR å¥ç´¢å¼•

    # 8ï¸âƒ£ æ ¹æ®åŒ¹é…å…³ç³»è®¡ç®—æ¯å¯¹çš„ CERï¼Œå¹¶æ„å»ºåŒ¹é…ç»“æœ
    matched_ref = [reference_texts[i] for i in row_ind]
    matched_asr = [asr_texts[j] for j in col_ind]
    cer_list = [cost_matrix[i, j] for i, j in zip(row_ind, col_ind)]

    avg_cer = np.mean(cer_list) if cer_list else 0

    # 9ï¸âƒ£ ç”Ÿæˆ CER ç»“æœæ–‡ä»¶
    # ä¿å­˜åŒ¹é…å…³ç³»æ—¶ï¼Œuttid å– ASR ç»“æœå¯¹åº”çš„ uttidï¼ˆæ ¹æ® col_ind é¡ºåºæ’åºï¼‰
    matched_uttid = [uttid_list[j] for j in col_ind]
    cer_csv = os.path.join(output_dir, f"{sub_dir}_cer.csv")
    compare_df = pd.DataFrame({
        "uttid": matched_uttid,
        "reference": matched_ref,
        "transcription": matched_asr,
        "CER": cer_list
    })
    compare_df.to_csv(cer_csv, index=False)

    # ğŸ”Ÿ è¾“å‡ºæœ€ç»ˆ CER ç»“æœ
    print(f"ğŸ“Š {sub_dir} è¯†åˆ«ç»“æœ:")
    print(f"âœ… å¹³å‡ å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰: {avg_cer:.2%}")
    print(f"ğŸ“‚ è¯¦ç»†å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ° {cer_csv}")

print("ğŸ‰ æ‰€æœ‰ ASR ç»“æœå¤„ç†å®Œæˆï¼")